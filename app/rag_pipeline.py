import os
import re
import uuid
import torch
import tiktoken

from datetime import date, datetime
from typing import Any

from app.config import settings
from app.router.clova_router import Executor
from app.clients.hyperclova_client import ask_hyperclova
from app.utils.ticker_map import extract_tickers_from_query, all_tickers
from app.risk.volatility import get_volatility_info
from app.risk.beta import get_beta_info

from app.chunking.chunker import (
    semantic_section_chunk,
    topic_model_chunk,
    sliding_window_chunk,
    chunk_file,
)
from app.retrieval.vectorstore import (
    upsert_document,
    build_bm25_index,
    bm25_search,
    hybrid_search,
    dense_collection,
    _bm25,
    _bm25_ids,
)

from sentence_transformers import CrossEncoder

import numpy as np
from sklearn.preprocessing import minmax_scale
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction
from konlpy.tag import Okt


#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Device setup for Apple MPS / CUDA / CPU
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if torch.backends.mps.is_available():
    device = "mps"
elif torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Cross-encoder setup
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cross_encoder = CrossEncoder(
    "cross-encoder/ms-marco-TinyBERT-L-2",
    device=device
)

def rerank_with_cross_encoder(query: str, candidates: list[str], top_k: int) -> list[str]:
    pairs = [[query, c] for c in candidates]
    scores = cross_encoder.predict(pairs)
    ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
    return [text for text, _ in ranked[:top_k]]

def date_from_id(doc_id: str) -> date | None:
    try:
        return datetime.strptime(doc_id.split("_", 1)[0], "%Y-%m-%d").date()
    except ValueError:
        return None

def parse_date_from_text(text: str) -> date | None:
    m = re.search(r"(\d{4})[.\-/](\d{1,2})[.\-/](\d{1,2})", text)
    if not m:
        return None
    y, mo, d = map(int, m.groups())
    try:
        return date(y, mo, d)
    except ValueError:
        return None

class RAGPipeline:
    def __init__(
        self,
        chunk_method: str = "section",
        bm25_k: int = 75,
        dense_k: int = 75,
        final_k: int = 7,
        dense_weight: float = 0.4,
        sparse_weight: float = 0.4,
        recency_weight: float = 0.5,
        max_tokens: int = 500,
        overlap: int = 50,
        ticker_list: list[str] | None = None,
    ):
        # retrieval settings
        self.bm25_k         = bm25_k
        self.dense_k        = dense_k
        self.final_k        = final_k
        self.dense_weight   = dense_weight
        self.sparse_weight  = sparse_weight
        self.recency_weight = recency_weight

        # chunking configuration
        self.chunk_method = chunk_method
        self.chunk_kwargs = {"max_tokens": max_tokens, "overlap": overlap}

        # initialize CLOVA Router
        self.router = Executor(
            host=os.getenv("CLOVA_HOST", "clovastudio.stream.ntruss.com"),
        )

        # initialize ticker list
        self.ticker_list = ticker_list if ticker_list is not None else all_tickers()

    def ingest(self, doc_id: str, text: str) -> None:
        if self.chunk_method == "section":
            chunks = semantic_section_chunk(text) or sliding_window_chunk(text, **self.chunk_kwargs)
        elif self.chunk_method == "topic":
            chunks = topic_model_chunk(text, **self.chunk_kwargs) or sliding_window_chunk(text, **self.chunk_kwargs)
        else:
            chunks = sliding_window_chunk(text, **self.chunk_kwargs)

        for i, c in enumerate(chunks):
            if not isinstance(c, str) or not c.strip():
                continue
            upsert_document(f"{doc_id}_{i}", c)

    def ingest_file(self, file_path: str) -> None:
        model = settings.EMBEDDING_MODEL_NAME
        enc   = tiktoken.encoding_for_model(model)

        chunks    = chunk_file(file_path, **self.chunk_kwargs)
        base_name = os.path.splitext(os.path.basename(file_path))[0]
        max_tok   = self.chunk_kwargs["max_tokens"]

        for i, chunk in enumerate(chunks):
            if not isinstance(chunk, str) or not chunk.strip():
                continue

            tok_count = len(enc.encode(chunk))
            if tok_count > max_tok:
                sub_chunks = sliding_window_chunk(chunk, **self.chunk_kwargs)
            else:
                sub_chunks = [chunk]

            for j, sc in enumerate(sub_chunks):
                if not sc.strip():
                    continue

                doc_id = f"{base_name}_{i}_{j}"
                try:
                    upsert_document(doc_id, sc)
                except Exception as e:
                    msg = str(e)
                    m = re.search(
                        r"maximum context length is (\d+) tokens.*requested (\d+)",
                        msg,
                    )
                    if m:
                        limit = int(m.group(1)) // 2
                        retry_chunks = sliding_window_chunk(
                            sc,
                            max_tokens=limit,
                            overlap=self.chunk_kwargs["overlap"],
                            model_name=model,
                        )
                        for k, rc in enumerate(retry_chunks):
                            if rc.strip():
                                try:
                                    upsert_document(f"{doc_id}_r{k}", rc)
                                except Exception:
                                    pass
                    else:
                        print(f"âš ï¸ Skipping chunk {doc_id}: {msg}")

    def finalize(self) -> None:
        """
        After all ingest()/ingest_file() calls are done,
        build the BM25 index exactly once.
        """
        build_bm25_index()

    def retrieve(self, query: str) -> list[tuple[str, str]]:
        # 0) Which tickers did the user mention?
        tickers = extract_tickers_from_query(query)

        # 1) BM25 shortlist
        bm25_ids = bm25_search(query, k=self.bm25_k)

        # 2) Denseâ€vector fallback
        dense_ids = hybrid_search(
            query,
            bm25_k=self.bm25_k,
            dense_k=self.dense_k,
            dense_weight=self.dense_weight,
            sparse_weight=self.sparse_weight,
        )

        # 3) Intersection (or fallback)
        candidates = [did for did in bm25_ids if did in dense_ids] or bm25_ids
        candidates = candidates[: self.final_k * 3]

        # 4) Fetch docs + embeddings
        resp = dense_collection.get(ids=candidates, include=["documents", "embeddings"])
        ids, docs, embs = resp["ids"], resp["documents"], resp["embeddings"]

        # 5) Feature computation
        today   = date.today()
        tokens  = Okt().morphs(query)
        # now _bm25 is freshly looked up from the vectorstore module
        sparse_scores = _bm25.get_scores(tokens) if _bm25 is not None else [0.0] * len(ids)
        sparse_map    = dict(zip(_bm25_ids, sparse_scores))

        # Query embedding
        q_vec = np.array(
            OpenAIEmbeddingFunction(
                api_key=settings.OPENAI_API_KEY,
                model_name=settings.EMBEDDING_MODEL_NAME
            )([query])[0]
        )

        raw_feats = []
        for did, text, emb in zip(ids, docs, embs):
            # recency
            from app.rag_pipeline import date_from_id, parse_date_from_text
            chunk_date = date_from_id(did) or parse_date_from_text(text) or date.min
            days_old   = (today - chunk_date).days
            recency    = max(0.0, 1.0 - days_old / 365.0)

            bm25_s = sparse_map.get(did, 0.0)
            arr    = np.array(emb)
            denom  = np.linalg.norm(arr) * np.linalg.norm(q_vec)
            dense_s = float(arr.dot(q_vec) / denom) if denom > 0 else 0.0

            raw_feats.append((did, text, bm25_s, dense_s, recency))

        def normalize(xs: list[float]) -> list[float]:
            return list(minmax_scale(xs)) if len(xs) > 1 else [1.0] * len(xs)

        _, _, bm_vals, dn_vals, rc_vals = zip(*raw_feats)
        norm_bm, norm_dn, norm_rc = normalize(bm_vals), normalize(dn_vals), normalize(rc_vals)

        # 6) Score & pick topâ€K
        scored = []
        for i, (did, text, *_ ) in enumerate(raw_feats):
            score = (
                self.sparse_weight  * norm_bm[i] +
                self.dense_weight   * norm_dn[i] +
                self.recency_weight * norm_rc[i]
            )

            if re.search(r"íˆ¬ì\s*ì˜ê²¬", text):
                score += 0.1

            scored.append((did, text, score))

        scored.sort(key=lambda x: x[2], reverse=True)
        top = scored[: self.final_k]


        # 7) Debug print & return
        print(f"\nğŸ” Retrieval for â€œ{query}â€ returned {len(top)} chunks:")
        results = []
        for did, text, *rest in top:
            base    = did.rsplit("_", 2)[0]
            snippet = text.replace("\n", " ")[:50]
            print(f"  â€¢ {did} (file: {base}): â€œ{snippet}â€¦â€")
            results.append((did, text))

        return results



    def answer(
        self,
        query: str,
        history: list[dict[str, Any]] | None = None
    ) -> str:
        """
        Fully LLM-driven answers with router-based domains:
        - Use routerâ€™s domain for classification into ìœ„í—˜ ì§€í‘œ, ì¢…ëª© ìœ„í—˜ ë¶„ì„, ìµœì‹  ì¢…ëª© ë‰´ìŠ¤, other => ì¼ë°˜ ê²€ìƒ‰
        - ìœ„í—˜ ì§€í‘œ: show only calculated volatility & beta
        - ì¢…ëª© ìœ„í—˜ ë¶„ì„: inject metrics + five fixed risk categories with citations
        - ìµœì‹  ì¢…ëª© ë‰´ìŠ¤: latest news template with citations
        - ì¼ë°˜ ê²€ìƒ‰: generic RAG retrieval and citations
        """

        # 1) extract tickers & build retrieval query
        tickers = extract_tickers_from_query(query)
        retrieval_q = f"{' '.join(tickers)} {query}" if tickers else query

        # 2) compute metrics
        metrics: dict[str, dict[str, Any]] = {}
        if tickers:
            t0 = tickers[0]
            vol = get_volatility_info(t0)
            is_vol_elevated = vol.get("risk_level", "N/A").lower() in ("high", "severe") or vol["volatility"] > 0.20
            metrics["volatility"] = {
                "label": f"ì—°ê°„ ë³€ë™ì„±: {vol['volatility']:.2%} (risk level: {vol.get('risk_level','N/A')})",
                "elevated": is_vol_elevated
            }
            b = get_beta_info(t0)
            interp = f" ({b.get('interpretation')})" if b.get("interpretation") else ""
            is_beta_elevated = abs(b["beta"] - 1.0) > 0.20
            metrics["beta"] = {
                "label": f"ë² íƒ€: {b['beta']:.2f}{interp}",
                "elevated": is_beta_elevated
            }
            metrics["de_ratio"] = {"label": "D/E ë¹„ìœ¨: {de_ratio}", "elevated": False}
            metrics["evi"]      = {"label": "EVI: {evi}",      "elevated": False}
            metrics["ccr"]      = {"label": "CCR: {ccr}",      "elevated": False}

        # 3) domain routing via router
        payload = {"query": query, "chatHistory": history or []}
        domain = self.router.execute(payload).get("domain", "ì¼ë°˜ ê²€ìƒ‰")  # Direct use of self.router.execute()

        # debug
        print(f"[Debug] router domain: '{domain}'")

        # 4) ìœ„í—˜ ì§€í‘œ: metrics only
        if domain == "ìœ„í—˜ ì§€í‘œ":
            if not tickers:
                return "ê¶ê¸ˆí•´í•˜ì‹œëŠ” ì¢…ëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”"
            header_lines = ["(Retrieved from calculation)", f"Ticker: {t0}"]
            header_lines.append(f"- {metrics['volatility']['label']}")
            header_lines.append(f"- {metrics['beta']['label']}")
            return "\n".join(header_lines)

        # 5) ì¢…ëª© ìœ„í—˜ ë¶„ì„ without ticker
        if domain == "ì¢…ëª© ìœ„í—˜ ë¶„ì„" and not tickers:
            return "ê¶ê¸ˆí•´í•˜ì‹œëŠ” ì¢…ëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”"

        # 6) prepare metric injection for risk domains
        metric_texts: list[str] = []
        header = ""
        if domain in ("ì¢…ëª© ìœ„í—˜ ë¶„ì„", "ìœ„í—˜ ì§€í‘œ") and tickers:
            metric_texts = [
                f"(Calculated) {metrics['volatility']['label']}",
                f"(Calculated) {metrics['beta']['label']}"
            ]
            if domain == "ì¢…ëª© ìœ„í—˜ ë¶„ì„":
                header_lines = ["(Retrieved from calculation)", f"Ticker: {t0}"]
                if metrics['volatility']['elevated']:
                    header_lines.append(f"- {metrics['volatility']['label']}")
                if metrics['beta']['elevated']:
                    header_lines.append(f"- {metrics['beta']['label']}")
                header = "\n".join(header_lines) + "\n\n"

        # 7) carry forward last two turns
        prefix = ""
        if history:
            for turn in history[-2:]:
                prefix += f"Q: {turn['question']}\nA: {turn['answer']}\n\n"

        # 8) retrieval and rerank with citations
        candidates = self.retrieve(retrieval_q)
        text_to_src = {txt: src for src, txt in candidates}
        base_texts = [txt for _, txt in candidates]
        combined = metric_texts + base_texts if domain in ("ì¢…ëª© ìœ„í—˜ ë¶„ì„", "ìœ„í—˜ ì§€í‘œ") else base_texts
        k = self.final_k * 2 if domain in ("ì¢…ëª© ìœ„í—˜ ë¶„ì„", "ìœ„í—˜ ì§€í‘œ") else self.final_k
        top_texts = rerank_with_cross_encoder(retrieval_q, combined, k)
        numbered_lines = []
        for i, txt in enumerate(top_texts, 1):
            src = text_to_src.get(txt, "ê³„ì‚°ê°’")
            numbered_lines.append(f"[{i}] {txt} [ì¶œì²˜:{src}]")
        numbered = "\n\n".join(numbered_lines)

        # 9) build prompts
        recent_chunks = self.retrieve(retrieval_q)
        numbered_recent = []
        for idx, (_, t) in enumerate(recent_chunks, 1):
            numbered_recent.append(f"[{idx}] {t} [ì¶œì²˜:{recent_chunks[idx-1][0]}]")
        numbered_recent = "\n\n".join(numbered_recent)
        recent_tmpl = (
            f"ì•„ë˜ ìë£Œë§Œ ë³´ê³  '{query}'ì˜ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.\n"
            "- ëª¨ë“  ìˆ«ìëŠ” [ì¶œì²˜ë²ˆí˜¸] í˜•íƒœë¡œ í‘œì‹œí•´ì£¼ì„¸ìš”.\n"
            "- ìë£Œì™€ ì˜ê²¬ì˜ ë‚ ì§œ ë˜í•œ í‘œì‹œí•˜ì„¸ìš”.\n"
            "- í•µì‹¬ í¬ì¸íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ\n\n"
            + numbered_recent +
            f"\n\nQ: {query}\nA:"
        )

        def cat_chunks(cat):
            chunks = self.retrieve(f"{query} {cat}")
            return "".join(f"   - {t}\n" for _, t in chunks)

        risk_tmpl = (
            f"ì•„ë˜ì— ì œê³µëœ ì¦ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{query}'ì˜ ìœ„í—˜ ìš”ì¸ì„ ë‹¤ì„¯ ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ë‚˜ëˆ„ì–´ í•œêµ­ì–´ë¡œ ìƒì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”.\n"
            "- ê° ì„¹ì…˜ ì œëª©ì€ ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì´ ì •í™•íˆ ì‚¬ìš©í•  ê²ƒ:\n"
            "  1. ì‹œì¥ ë¦¬ìŠ¤í¬\n"
            "  2. ì¬ë¬´ ë¦¬ìŠ¤í¬\n"
            "  3. ì‚¬ì—… ë¦¬ìŠ¤í¬\n"
            "  4. ë²•ë¥ /ìš´ì˜ ë¦¬ìŠ¤í¬\n"
            "  5. ESG/í‰íŒ ë¦¬ìŠ¤í¬\n"
            "- ìœ„ ì™¸ì— ë‹¤ë¥¸ í‘œí˜„ ê¸ˆì§€\n"
            "- ëª¨ë“  ìˆ«ìëŠ” [ì¶œì²˜ë²ˆí˜¸] í˜•íƒœë¡œ í‘œì‹œí•´ì£¼ì„¸ìš”.\n"
            "- ê°€ëŠ¥í•œ í•œ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„  ë°˜ì˜í•˜ì„¸ìš”.\n\n"
            "1. ì‹œì¥ ë¦¬ìŠ¤í¬\n" + cat_chunks("ì‹œì¥ ë¦¬ìŠ¤í¬") +
            "\n2. ì¬ë¬´ ë¦¬ìŠ¤í¬\n" + cat_chunks("ì¬ë¬´ ë¦¬ìŠ¤í¬") +
            "\n3. ì‚¬ì—… ë¦¬ìŠ¤í¬\n" + cat_chunks("ì‚¬ì—… ë¦¬ìŠ¤í¬") +
            "\n4. ë²•ë¥ /ìš´ì˜ ë¦¬ìŠ¤í¬\n" + cat_chunks("ìš´ì˜ ë¦¬ìŠ¤í¬") +
            "\n5. ESG/í‰íŒ ë¦¬ìŠ¤í¬\n" + cat_chunks("ESG í‰íŒ ë¦¬ìŠ¤í¬") +
            f"\n\n{numbered}\n\nQ: {query}\nA:"
        )

        # 10) assemble prompt
        if domain == "ì¢…ëª© ìœ„í—˜ ë¶„ì„":
            prompt = prefix + risk_tmpl
        elif domain == "ìµœì‹  ì¢…ëª© ë‰´ìŠ¤":
            prompt = prefix + recent_tmpl
        else:
            prompt = prefix + numbered + f"\n\nQ: {query}\nA:"

        # 11) call LLM
        response = ask_hyperclova(prompt)

        # 12) return
        return (header + response) if domain == "ì¢…ëª© ìœ„í—˜ ë¶„ì„" else response


